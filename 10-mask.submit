#! /bin/bash

# Ensure we're running from the correct location
CWD_check()
{
	#local SCRIPTS_DIR
	local MAIN_DIR
	local RUN_DIR

	SCRIPTS_DIR=$(readlink -f `dirname "${BASH_SOURCE[0]}"`)
	MAIN_DIR=$(readlink -f `dirname "${SCRIPTS_DIR}/"`)
	RUN_DIR=$(readlink -f .)

	if [ "${RUN_DIR}" != "${MAIN_DIR}" ] || ! [[ "${SCRIPTS_DIR}" =~ ^"${MAIN_DIR}"/scripts.* ]]
	then
		printf "\n\t%s\n\t%s\n\n" "Script must be run from ${MAIN_DIR}" "You are currently at:   ${RUN_DIR}" 1>&2
		exit 1
	fi
}
CWD_check

rangify()
{
	local a b FIRST IFS 
	local -a RANGES
	local RANGE_END=0


	while [ $# -ge 1 ]
	do
		a=$((10#${1}))
		shift
		b=$((10#${1}))

		if [[ ${a}+1 -eq ${b} ]]
		then
			if [ ${RANGE_END} -eq 0 ]
			then
				FIRST=${a}
				RANGE_END=1
			fi
		else
			if [ ${RANGE_END} -eq 1 ]
			then
				RANGES+=(${FIRST}-${a})
			else
				RANGES+=(${a})
			fi
			RANGE_END=0
		fi

	done

	IFS=','
	echo "${RANGES[*]}"
}

submitJob()
{
	local JOB_NAME ASM_FA ALN_BAM DEPTH ASM_SEQ_IDS_LIST OUT_PFX ARRAY_JOBS JOBS_AT_ONCE
	JOB_NAME="${1}"
	ASM_FA="${2}"
	ALN_BAM="${3}"
	DEPTH="${4}"
	ASM_SEQ_IDS_LIST="${5}"
	OUT_PFX="${6}"
	ARRAY_JOBS="${7}"
	JOBS_AT_ONCE="${8}"

	# computed resource requirements based on first two scaffolds of BFT, one of which is in the top 3 for length.
	# These tests were done with 16 cores and 64 GB RAM available. Only 1 core was used on average. We could try it with one core,
	# but 2 may be required to avoid a bottlneck during decompression. We'll test with 2 and see what happens next. Update: I tested
	# with 2 cores and the jobs were consistently ~50% utilization. I think it is safe to use only 1 core.
	# memory requirements: y = 1.17372457486x + 10.535 <-- y: RAM in GB, x: seq in Mbp
	#   time requirements: y = 1.069801045x - 0.647757586 <-- y: time in hours, x: seq in Mbp
	# the problem with array jobs is that I can't determine the resources dynamically. I should really use array jobs in the future only when the data chunks are of equal size.

	# longest seq for BFT is ~8.933 Mbp. So TIME > 8.91 hours and RAM > 21.02 GB.

	sbatch \
		-J ${JOB_NAME} \
		--signal=B:USR1@300 \
		--array=${ARRAY_JOBS}%${JOBS_AT_ONCE} \
		--time=0-10:00:00 \
		--ntasks=1 \
		--nodes=1 \
		--mem=22G \
		-o job_files/%x__%A-%a.out \
		-e job_files/%x__%A-%a.err \
		${SCRIPTS_DIR}/10-mask.slurm \
		"${ASM_FA}" \
		"${ALN_BAM}" \
		"${DEPTH}" \
		"${ASM_SEQ_IDS_LIST}" \
		"${OUT_PFX}" \
		"${OUT_BED_SFX}" \
		"${OUT_VCF_SFX}"
}

# ###################################### #
# sanity check on input and output files #
# ###################################### #

# define key variables
DEBUG=0 # 0: False, 1: True
SPECIES="bft"
PROJECT="${SPECIES}-msmc"

JOBS_TO_RUN_AT_ONE_TIME=100 # total across all samples
JOBS_TO_RUN_AT_ONE_TIME_PER_SAMPLE=${JOBS_TO_RUN_AT_ONE_TIME} # default value if only one sample

OUTPUT_PFX="data/masks/mask_"
OUTPUT_DIR=$(readlink -n -m `dirname "${OUTPUT_PFX}"`)
OUT_BED_SFX=".bed.gz"
OUT_VCF_SFX=".vcf.gz"

SAMPLE_LIST="data/samples.list"
ASSEMBLY_FA="data/assembly/asm_long.fa"
ASSEMBLY_FAI="${ASSEMBLY_FA}.fai"
ASSEMBLY_SEQIDS="${ASSEMBLY_FAI}"

# check for the presence of input files that are not sample-specific
declare -a INPUT_FILES=("${ASSEMBLY_FA}" "${ASSEMBLY_FAI}" "${SAMPLE_FILE}")
for INPUT_FILE in "${INPUT_FILES[@]}"
do
	if [ ! -e "${INPUT_FILE}" ]
	then
		printf "%s\n\t%s\n" "ERROR: Required input file does not exist: ${INPUT_FILE}" "None of the samples can be run without this file." 1>&2
		exit 1
	fi
done
unset INPUT_FILES

# create output dir (if needed)
mkdir -p "${OUTPUT_DIR}" &> /dev/null

# calculate number of jobs to run at once _per sample_
NUM_SAMPLES=`cat "${SAMPLES_LIST}" | wc -l`
if [ "${NUM_SAMPLES}" -gt 1 ]
then
	if [ "${NUM_SAMPLES}" -ge ${JOBS_TO_RUN_AT_ONE_TIME} ]
	then
		JOBS_TO_RUN_AT_ONE_TIME_PER_SAMPLE=1
	else
		JOBS_TO_RUN_AT_ONE_TIME_PER_SAMPLE=$(( ${JOBS_TO_RUN_AT_ONE_TIME} / ${NUM_SAMPLES} ))
	fi
fi

# set some variables to keep track of the individual jobs (one per sample)
declare -a STARTED # list of samples
declare -a STARTED_RANGES # list of ranges
declare -a MISSING_INPUT # list of samples
declare -a NEEDED_INPUT_FILES # list of files
declare -a RESTARTED # list of samples
declare -a RESTARTED_RANGES # list of ranges
declare -a FINISHED # list of samples
declare -a FINISHED_RANGES # list of ranges (only the finished ones from samples that had to be restarted, not 100% completed samples)
declare -a COMPLETED # list of samples

while read SAMPLE
do
	SKIP=0

	ALIGNMENT_BAM="data/alns/${SAMPLE}_pacbio-reads_x_asm-long.bam"
	ALIGNMENT_BAI="${ALIGNMENT_BAM}.bai"
	AVE_DEPTHS_FILE="data/alns/${SAMPLE}_approx-depth.txt"
	SAMPLE_OUTPUT_PFX="${OUTPUT_PFX}${SAMPLE}_"

	declare -a INPUT_FILES=("${ALIGNMENT_BAM}" "${ALIGNMENT_BAI}" "${AVE_DEPTHS_FILE}")

	# check for existence of needed input files
	for INPUT_FILE in "${INPUT_FILES[@]}"
	do
		if [ ! -e "${INPUT_FILE}" ]
		then
			SKIP=1
			MISSING_INPUT+=("${SAMPLE}")
			NEEDED_INPUT_FILES+=("${INPUT_FILE}")
		fi
	done
	unset INPUT_FILES

	# read in the average depth
	AVE_DEPTH=`cat "${AVE_DEPTHS_FILE}" | tr -d '\n'`

	# loop through asm seqids to find output files
	declare -a TO_RUN
	declare -a ALREADY_FINISHED
	declare -a ALREADY_FINISHED_FILES
	declare -a ATTEMPTED
	COUNTER=1
	while read SEQID
	do
		BED="${SAMPLE_OUTPUT_PFX}${SEQID}${OUT_BED_SFX}"
		VCF="${SAMPLE_OUTPUT_PFX}${SEQID}${OUT_VCF_SFX}"
		BED_OK="${BED}.ok"
		VCF_OK="${VCF}.ok"

		if [[ -e "${BED}" ]] || [[ -e "${VCF}" ]] || [[ -e "${BED_OK}" ]] || [[ -e "${VCF_OK}" ]]
		then
			if [[ -e "${BED_OK}" ]] && [[ -e "${VCF_OK}" ]]
			then
				ALREADY_FINISHED+=("${COUNTER}")
				ALREADY_FINISHED_FILES+=("${BED}" "${BED_OK}" "${VCF}" "${VCF_OK}")
			else
				rm -f "${BED}" "${VCF}" "${BED_OK}" "${VCF_OK}" &> /dev/null
				TO_RUN+=("${COUNTER}")
				ATTEMPTED+=("${COUNTER}")
			fi
		else
			TO_RUN+=("${COUNTER}")
		fi

		COUNTER=$((${COUNTER}+1))

	done < <(cut -d '	' -f 1 "${ASSEMBLY_SEQIDS}")

	# set to skip job submission if range is empty
	if [ ${#TO_RUN[@]} -eq 0 ]
	then
		SKIP=1
		COMPLETED+=("${SAMPLE}")
	else
		STARTED+=("${SAMPLE}")

		# report on attempted / restarted
		if [ ${#ATTEMPTED[@]} -ne 0 ]
		then
			RANGE=`rangify "${ATTEMPTED[@]}"`
			if [ ${DEBUG} -eq 1 ]
			then
				printf "%s\n" "INFO: ${RANGE} has/have been attempted (1+ output files existed, but it hadn't" "finished running). These files were deleted so we could try again." 1>&2
			fi
			RESTARTED+=("${SAMPLE}")
			RESTARTED_RANGES+=("${RANGE}")
			unset RANGE
		fi

		# report on finished
		if [ ${#ALREADY_FINISHED[@]} -ne 0 ]
		then
			RANGE=`rangify "${ALREADY_FINISHED[@]}"`
			if [ ${DEBUG} -eq 1 ]
			then
				printf "%s\n" "INFO: ${RANGE} has/have already finished. We will skip it/them. To run them" "instead, first execute:" 1>&2
				printf "\t%s " "rm -f" 1>&2
				printf '"%s" ' "${ALREADY_FINISHED_FILES[@]}" 1>&2
				printf "\n" 1>&2
			fi
			FINISHED+=("${SAMPLE}")
			FINISHED_RANGES+=("${RANGE}")
			unset RANGE
		fi
	fi

	# submit or skip
	if [ ${SKIP} -eq 0 ]
	then
		# create array submission ranges string
		ARRAY_RANGE=`rangify "${TO_RUN[@]}"`

		STARTED+=("${SAMPLE}")
		STARTED_RANGES+=("${ARRAY_RANGE}")

		# ####################### #
		# actually submit the job #
		# ####################### #
		HPC_JOB_NAME="${PROJECT}_mask_${SAMPLE}"
		submitJob \
			"${HPC_JOB_NAME}" \
			"${ASSEMBLY_FA}" \
			"${ALIGNMENT_BAM}" \
			"${AVE_DEPTH}" \
			"${ASSEMBLY_SEQIDS}" \
			"${SAMPLE_OUTPUT_PFX}" \
			"${ARRAY_RANGE}" \
			"${JOBS_TO_RUN_AT_ONE_TIME_PER_SAMPLE}"
	else
		if [ ${DEBUG} -eq 1 ]
		then
			printf "%s\n" "INFO: No job was submitted for ${SAMPLE}." 1>&2
		fi
	fi
	unset SKIP TO_RUN ALREADY_FINISHED ALREADY_FINISHED_FILES ATTEMPTED ALIGNMENT_BAM ALIGNMENT_BAI AVE_DEPTHS_FILE HPC_JOB_NAME

done < "${SAMPLES_LIST}" # end of: while read SAMPLE

# ############## #
#     REPORT     #
# ############## #

# remove duplicates
MISSING_INPUT=(`printf '%s\n' "${MISSING_INPUT[@]}" | uniq | tr '\n' ' '`) # order preserved, assumes duplicates will always be adjacent

# report
#	completed
if [ ${#COMPLETED[@]} -gt 0 ]
then
	printf "%s\n" "The following samples had already been completed, thus no jobs were submitted:" 1>&2
	printf "\t%s\n" "${COMPLETED[@]}" 1>&2
	printf "\n%s\n" "####################" 1>&2
fi

#	unstarted
if [ ${#MISSING_INPUT[@]} -gt 0 ]
then
	printf "%s\n" "The following samples were not started because they were missing one or more input file:" 1>&2
	printf "\t%s\n" "${MISSING_INPUT[@]}" 1>&2
	printf "%s\n" "The following is a list of the files that were missing:" 1>&2
	printf "\t%s\n" "${MISSING_INPUT_FILES[@]}" 1>&2
	printf "\n%s\n" "####################" 1>&2
fi

#	started
if [ ${#STARTED[@]} -gt 0 ]
then

	# started scaffolds for this sample
	printf "%s\n" "The following shows which scaffolds were started for each sample:" 1>&2
	for i in `seq 0 1 ${#STARTED[@]}`
	do
		printf "\t%s: %s\n" "${STARTED[${i}]}" "${STARTED_RANGES[${i}]}" 1>&2
	done
	printf "\n%s\n" "####################" 1>&2

	# already finished scaffolds for this sample
	if [ ${#FINISHED[@]} -gt 0 ]
	then
		printf "%s\n" "Of these samples that were started, the following did not have to run certain scaffolds because they had already been completed:" 1>&2
		for i in `seq 0 1 ${#FINISHED[@]}`
		do
			printf "\t%s: %s\n" "${FINISHED[${i}]}" "${FINISHED_RANGES[${i}]}" 1>&2
		done
		printf "\n%s\n" "####################" 1>&2
	fi

	# restarted scaffolds for this sample
	if [ ${#RESTARTED[@]} -gt 0 ]
	then
		printf "%s\n" "Of these samples that were started, the following had scaffolds that were previously run but did not complete successfully for some reason:" 1>&2
		for i in `seq 0 1 ${#RESTARTED[@]}`
		do
			printf "\t%s: %s\n" "${RESTARTED[${i}]}" "${RESTARTED_RANGES[${i}]}" 1>&2
		done
		printf "\n%s\n" "####################" 1>&2
	fi
fi
